"""
58åŒåŸåŒ—äº¬äºŒæ‰‹æˆ¿ä¿¡æ¯çˆ¬è™«
"""
import requests
from bs4 import BeautifulSoup
import time
import random
import csv
import json
from urllib.parse import urljoin, urlparse, parse_qs
import re


class Spider58:
    def __init__(self):
        self.base_url = "https://bj.58.com/ershoufang/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://bj.58.com/ershoufang/',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.houses = []

    def get_page(self, url, retry=3):
        """è·å–ç½‘é¡µå†…å®¹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for i in range(retry):
            try:
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
                time.sleep(random.uniform(1, 3))
                
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯ç é¡µé¢
                if 'éªŒè¯ç ' in response.text or 'captcha' in response.url.lower():
                    print(f"âš ï¸  é‡åˆ°éªŒè¯ç ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´...")
                    time.sleep(random.uniform(5, 10))
                    continue
                
                if response.status_code == 200:
                    return response.text
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                if i < retry - 1:
                    time.sleep(random.uniform(2, 5))
        
        return None

    def parse_house_list(self, html):
        """è§£ææˆ¿æºåˆ—è¡¨é¡µé¢"""
        soup = BeautifulSoup(html, 'html.parser')
        houses = []
        
        # 58åŒåŸçš„æˆ¿æºä¿¡æ¯ä½¿ç”¨ class="property" çš„div
        items = soup.select('div.property')
        
        if not items:
            print("âš ï¸  æœªæ‰¾åˆ°æˆ¿æºåˆ—è¡¨ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨...")
            # å¤‡ç”¨é€‰æ‹©å™¨
            items = soup.find_all('div', class_='property')
        
        if items:
            print(f"âœ… æ‰¾åˆ° {len(items)} ä¸ªæˆ¿æº")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æˆ¿æºä¿¡æ¯")
        
        for item in items:
            try:
                house_info = self.extract_house_info(item)
                if house_info:
                    houses.append(house_info)
            except Exception as e:
                print(f"âš ï¸  è§£æå•ä¸ªæˆ¿æºæ—¶å‡ºé”™: {e}")
                continue
        
        return houses

    def extract_house_info(self, item):
        """ä»å•ä¸ªæˆ¿æºå…ƒç´ ä¸­æå–ä¿¡æ¯"""
        house_info = {}
        
        try:
            # æ ‡é¢˜ - ä½¿ç”¨ property-content-title-name
            title_elem = item.select_one('h3.property-content-title-name')
            if title_elem:
                house_info['title'] = title_elem.get_text(strip=True)
            else:
                # å¤‡ç”¨ï¼šä»titleå±æ€§è·å–
                title_elem = item.select_one('[title]')
                if title_elem:
                    house_info['title'] = title_elem.get('title', '').strip()
            
            # é“¾æ¥ - åœ¨aæ ‡ç­¾çš„hrefä¸­
            link_elem = item.select_one('a.property-ex, a[href*="ershoufang"]')
            if link_elem:
                href = link_elem.get('href', '')
                if href:
                    house_info['url'] = urljoin(self.base_url, href)
            
            # ä»·æ ¼ - property-price-total-num å’Œ property-price-total-text
            price_num_elem = item.select_one('span.property-price-total-num')
            price_text_elem = item.select_one('span.property-price-total-text')
            if price_num_elem:
                price_num = price_num_elem.get_text(strip=True)
                price_unit = price_text_elem.get_text(strip=True) if price_text_elem else 'ä¸‡'
                house_info['price'] = price_num
                house_info['price_unit'] = price_unit
            
            # å•ä»· - property-price-average
            avg_price_elem = item.select_one('p.property-price-average')
            if avg_price_elem:
                avg_price_text = avg_price_elem.get_text(strip=True)
                # æå–æ•°å­—éƒ¨åˆ†
                avg_match = re.search(r'(\d+(?:\.\d+)?)', avg_price_text)
                if avg_match:
                    house_info['price_per_sqm'] = avg_price_text
            
            # æˆ¿å‹ - property-content-info-attribute
            room_elem = item.select_one('p.property-content-info-attribute')
            if room_elem:
                room_spans = room_elem.find_all('span')
                room_parts = []
                for i in range(0, len(room_spans), 2):
                    if i + 1 < len(room_spans):
                        num = room_spans[i].get_text(strip=True)
                        unit = room_spans[i + 1].get_text(strip=True)
                        room_parts.append(f"{num}{unit}")
                if room_parts:
                    house_info['room_type'] = ''.join(room_parts)
            
            # é¢ç§¯ - åœ¨ property-content-info-text ä¸­æŸ¥æ‰¾åŒ…å«ã¡çš„
            info_texts = item.select('p.property-content-info-text')
            for info_text in info_texts:
                text = info_text.get_text(strip=True)
                # é¢ç§¯
                area_match = re.search(r'(\d+(?:\.\d+)?)\s*ã¡', text)
                if area_match and 'area' not in house_info:
                    house_info['area'] = area_match.group(1)
                
                # æœå‘
                if 'å—åŒ—' in text or 'ä¸œè¥¿' in text or 'å—' in text or 'åŒ—' in text:
                    if 'orientation' not in house_info:
                        house_info['orientation'] = text
                
                # æ¥¼å±‚ä¿¡æ¯
                if 'å±‚' in text and 'floor' not in house_info:
                    house_info['floor'] = text
                
                # å»ºé€ å¹´ä»½
                year_match = re.search(r'(\d{4})å¹´', text)
                if year_match and 'year' not in house_info:
                    house_info['year'] = year_match.group(1)
            
            # å°åŒºåç§° - property-content-info-comm-name
            comm_name_elem = item.select_one('p.property-content-info-comm-name')
            if comm_name_elem:
                house_info['community'] = comm_name_elem.get_text(strip=True)
            
            # åœ°å€ - property-content-info-comm-address
            address_elem = item.select_one('p.property-content-info-comm-address')
            if address_elem:
                address_spans = address_elem.find_all('span')
                address_parts = [span.get_text(strip=True) for span in address_spans]
                if address_parts:
                    house_info['location'] = ' '.join(address_parts)
            
            # æ ‡ç­¾ - property-content-info-tag
            tags = item.select('span.property-content-info-tag')
            if tags:
                house_info['tags'] = [tag.get_text(strip=True) for tag in tags]
            
            # å¦‚æœè‡³å°‘æå–åˆ°äº†æ ‡é¢˜ï¼Œå°±è®¤ä¸ºæˆåŠŸ
            if house_info.get('title'):
                return house_info
                
        except Exception as e:
            print(f"âš ï¸  æå–ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        return None

    def save_to_csv(self, filename='houses_58.csv'):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not self.houses:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å­—æ®µ
        all_fields = set()
        for house in self.houses:
            all_fields.update(house.keys())
        
        # å®šä¹‰å­—æ®µé¡ºåº
        fieldnames = ['title', 'price', 'price_unit', 'price_per_sqm', 'room_type', 'area', 
                     'orientation', 'floor', 'year', 'community', 'location', 'tags', 'url']
        # æ·»åŠ å…¶ä»–å­—æ®µ
        for field in sorted(all_fields):
            if field not in fieldnames:
                fieldnames.append(field)
        
        with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            for house in self.houses:
                # å°†åˆ—è¡¨ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                row = {}
                for key, value in house.items():
                    if isinstance(value, list):
                        row[key] = ' '.join(value)
                    else:
                        row[key] = value
                writer.writerow(row)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {filename}ï¼Œå…± {len(self.houses)} æ¡è®°å½•")

    def save_to_json(self, filename='houses_58.json'):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not self.houses:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.houses, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {filename}ï¼Œå…± {len(self.houses)} æ¡è®°å½•")

    def crawl(self, max_pages=5, use_local_html=False):
        """çˆ¬å–æŒ‡å®šé¡µæ•°çš„æ•°æ®
        
        Args:
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            use_local_html: å¦‚æœä¸ºTrueï¼Œä½¿ç”¨æœ¬åœ°58.htmlæ–‡ä»¶è¿›è¡Œæµ‹è¯•
        """
        if use_local_html:
            print("ğŸ“„ ä½¿ç”¨æœ¬åœ°HTMLæ–‡ä»¶è¿›è¡Œæµ‹è¯•...")
            try:
                with open('58.html', 'r', encoding='utf-8') as f:
                    html = f.read()
                houses = self.parse_house_list(html)
                if houses:
                    self.houses.extend(houses)
                    print(f"âœ… ä»æœ¬åœ°æ–‡ä»¶æˆåŠŸæå– {len(houses)} æ¡æˆ¿æºä¿¡æ¯")
                else:
                    print("âŒ ä»æœ¬åœ°æ–‡ä»¶æœªæ‰¾åˆ°æˆ¿æºä¿¡æ¯")
            except FileNotFoundError:
                print("âŒ æœªæ‰¾åˆ°58.htmlæ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°åœ¨çº¿çˆ¬å–æ¨¡å¼")
                use_local_html = False
        
        if not use_local_html:
            print(f"ğŸš€ å¼€å§‹çˆ¬å–58åŒåŸåŒ—äº¬äºŒæ‰‹æˆ¿ä¿¡æ¯ï¼Œè®¡åˆ’çˆ¬å– {max_pages} é¡µ...")
            
            for page in range(1, max_pages + 1):
                print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
                
                # æ„å»ºURLï¼ˆ58åŒåŸçš„åˆ†é¡µå‚æ•°é€šå¸¸æ˜¯PGTIDå’Œpageï¼‰
                if page == 1:
                    url = self.base_url
                else:
                    url = f"{self.base_url}pn{page}/"
                
                html = self.get_page(url)
                if not html:
                    print(f"âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                houses = self.parse_house_list(html)
                if houses:
                    self.houses.extend(houses)
                    print(f"âœ… ç¬¬ {page} é¡µæˆåŠŸæå– {len(houses)} æ¡æˆ¿æºä¿¡æ¯")
                else:
                    print(f"âš ï¸  ç¬¬ {page} é¡µæœªæ‰¾åˆ°æˆ¿æºä¿¡æ¯ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–")
                    # ä¿å­˜å½“å‰HTMLç”¨äºè°ƒè¯•
                    with open(f'debug_page_{page}.html', 'w', encoding='utf-8') as f:
                        f.write(html)
                    print(f"   å·²ä¿å­˜é¡µé¢HTMLåˆ° debug_page_{page}.html ä¾›æ£€æŸ¥")
                
                # é¡µé¢é—´å»¶è¿Ÿ
                if page < max_pages:
                    delay = random.uniform(3, 6)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                    time.sleep(delay)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.houses)} æ¡æˆ¿æºä¿¡æ¯")
        
        # ä¿å­˜æ•°æ®
        if self.houses:
            self.save_to_csv()
            self.save_to_json()
            
            # æ‰“å°å‰å‡ æ¡æ•°æ®é¢„è§ˆ
            print("\nğŸ“Š æ•°æ®é¢„è§ˆï¼ˆå‰5æ¡ï¼‰ï¼š")
            for i, house in enumerate(self.houses[:5], 1):
                print(f"\n{i}. {house.get('title', 'N/A')}")
                print(f"   ä»·æ ¼: {house.get('price', 'N/A')}{house.get('price_unit', 'ä¸‡')}")
                if house.get('price_per_sqm'):
                    print(f"   å•ä»·: {house.get('price_per_sqm', 'N/A')}")
                print(f"   æˆ¿å‹: {house.get('room_type', 'N/A')}")
                print(f"   é¢ç§¯: {house.get('area', 'N/A')}ã¡")
                print(f"   æœå‘: {house.get('orientation', 'N/A')}")
                print(f"   å°åŒº: {house.get('community', 'N/A')}")
                print(f"   ä½ç½®: {house.get('location', 'N/A')}")
                if house.get('floor'):
                    print(f"   æ¥¼å±‚: {house.get('floor', 'N/A')}")
                if house.get('year'):
                    print(f"   å»ºé€ å¹´ä»½: {house.get('year', 'N/A')}")
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ï¼š")
            print("   1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("   2. ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
            print("   3. æ˜¯å¦è§¦å‘äº†åçˆ¬è™«æœºåˆ¶")
            print("   4. é¡µé¢ç»“æ„æ˜¯å¦å·²å˜åŒ–ï¼ˆå¯æŸ¥çœ‹debug_page_*.htmlæ–‡ä»¶ï¼‰")


def main():
    spider = Spider58()
    
    # å¦‚æœæœ¬åœ°æœ‰58.htmlæ–‡ä»¶ï¼Œå¯ä»¥å…ˆæµ‹è¯•æœ¬åœ°æ–‡ä»¶
    # spider.crawl(max_pages=1, use_local_html=True)
    
    # çˆ¬å–5é¡µæ•°æ®ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    spider.crawl(max_pages=5)


if __name__ == '__main__':
    main()

